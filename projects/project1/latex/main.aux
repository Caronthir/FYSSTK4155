\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\gdef \tocmax@section{18.6663pt}
\gdef \tocmax@subsection{14.91676pt}
\gdef \tocmax@subsubsection{12.27776pt}
\gdef \tocmax@paragraph{5.0pt}
\gdef \tocmax@appendix{5.0pt}
\gdef \tocmax@pagenum{10.0pt}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\newlabel{FirstPage@cref}{{}{[][1][]1}}
\@writefile{toc}{\contentsline {title}{ \relax \fontsize  {12}{14pt}\selectfont  \textsc  {FYSSTK4155} \@centercr   [25pt] \rule  {\linewidth }{0.5pt} \@centercr  [0.4cm] \relax \fontsize  {20.74}{25pt}\selectfont  Project 1 - Regression}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\tocdepth@munge}
\@writefile{toc}{\contentsline {section}{\numberline {}Contents}{1}{section*.3}\protected@file@percent }
\@writefile{toc}{\tocdepth@restore}
\citation{statelem}
\citation{statelem}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory}{2}{section*.5}\protected@file@percent }
\newlabel{sec:Theory}{{II}{2}{}{section*.5}{}}
\newlabel{sec:Theory@cref}{{[section][2][]II}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Ordinary Least Squares}{2}{section*.6}\protected@file@percent }
\newlabel{subsec:OLS}{{II\tmspace  +\thinmuskip {.1667em}A}{2}{}{section*.6}{}}
\newlabel{subsec:OLS@cref}{{[subsection][1][2]II\tmspace  +\thinmuskip {.1667em}A}{[1][2][]2}}
\newlabel{eq:1}{{{II.1}}{2}{}{AMS.8}{}}
\newlabel{eq:1@cref}{{[equation][2147483647][]{II.1}}{[1][2][]2}}
\citation{statelem}
\@writefile{lof}{\contentsline {figure}{\numberline {II.1}{\ignorespaces  Illustration of the principle of least squares. A sample of points are shown in both plots which are drawn from the true relationship (orange line) with some random noise. A linear model is guessed at, resulting in the blue line in the upper plot. The square error at each both is drawn as blue rectangles, the sum of their areas being RSS.\spacefactor \@m {} By minimizing this area we obtain the OLS estimate, shown in the lower plot.\relax }}{3}{figure.caption.9}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lq}{{II.1}{3}{Illustration of the principle of least squares. A sample of points are shown in both plots which are drawn from the true relationship (orange line) with some random noise. A linear model is guessed at, resulting in the blue line in the upper plot. The square error at each both is drawn as blue rectangles, the sum of their areas being RSS.\@ By minimizing this area we obtain the OLS estimate, shown in the lower plot.\relax }{figure.caption.9}{}}
\newlabel{fig:lq@cref}{{[figure][1][2]II.1}{[1][2][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Ridge Regularization}{3}{section*.10}\protected@file@percent }
\newlabel{subsec:Ridge}{{II\tmspace  +\thinmuskip {.1667em}B}{3}{}{section*.10}{}}
\newlabel{subsec:Ridge@cref}{{[subsection][2][2]II\tmspace  +\thinmuskip {.1667em}B}{[1][3][]3}}
\citation{statelem}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Lasso Regularization}{4}{section*.11}\protected@file@percent }
\newlabel{subsec:Lasso}{{II\tmspace  +\thinmuskip {.1667em}C}{4}{}{section*.11}{}}
\newlabel{subsec:Lasso@cref}{{[subsection][3][2]II\tmspace  +\thinmuskip {.1667em}C}{[1][3][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Bias Variance Tradeoff}{4}{section*.12}\protected@file@percent }
\newlabel{sec:bias-vari-trad}{{II\tmspace  +\thinmuskip {.1667em}D}{4}{}{section*.12}{}}
\newlabel{sec:bias-vari-trad@cref}{{[subsection][4][2]II\tmspace  +\thinmuskip {.1667em}D}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Cross Validation}{4}{section*.16}\protected@file@percent }
\newlabel{sec:cv}{{II\tmspace  +\thinmuskip {.1667em}E}{4}{}{section*.16}{}}
\newlabel{sec:cv@cref}{{[subsection][5][2]II\tmspace  +\thinmuskip {.1667em}E}{[1][4][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {II.2}{\ignorespaces  Monte Carlo simulations run on the same data set with different random noise using OLS regression. The mean square error is computed for increasing model complexity. The Monte Carlo mean is shown in thicker lines and darker colors. The training MSE falls off to zero as the model complexity increases, while the test error initially decreases until the models begin to overfit the training set and the test error increases. Note also the spreading out of test variance as the complexity increases.\relax }}{5}{figure.caption.13}\protected@file@percent }
\newlabel{fig:bias}{{II.2}{5}{Monte Carlo simulations run on the same data set with different random noise using OLS regression. The mean square error is computed for increasing model complexity. The Monte Carlo mean is shown in thicker lines and darker colors. The training MSE falls off to zero as the model complexity increases, while the test error initially decreases until the models begin to overfit the training set and the test error increases. Note also the spreading out of test variance as the complexity increases.\relax }{figure.caption.13}{}}
\newlabel{fig:bias@cref}{{[figure][2][2]II.2}{[1][4][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{5}{section*.17}\protected@file@percent }
\newlabel{sec:Method}{{III}{5}{}{section*.17}{}}
\newlabel{sec:Method@cref}{{[section][3][]III}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Simple Terrain Approximation}{5}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Real World Terrain Data}{5}{section*.20}\protected@file@percent }
\newlabel{sec:real-world-terrain}{{III\tmspace  +\thinmuskip {.1667em}B}{5}{}{section*.20}{}}
\newlabel{sec:real-world-terrain@cref}{{[subsection][2][3]III\tmspace  +\thinmuskip {.1667em}B}{[1][5][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {II.3}{\ignorespaces  Same Monte Carlo simulation as~\cref  {fig:bias} but showing the decomposition of the training error. The green lines show the Monte Carlo simulations of the training error minus the random noise. When the squared bias is added to the variance, the sum is exactly as the mean of the Monte Carlo simulations. As the random error is unknown, the actual training MSE lies slighty higher. The bias decreases as the model complexity increases as the model is better to explain the variance. However, as the each model fits the training data better and better, difference between the models increases, and so the variance increases.\relax }}{5}{figure.caption.14}\protected@file@percent }
\newlabel{fig:biastrain}{{II.3}{5}{Same Monte Carlo simulation as~\cref {fig:bias} but showing the decomposition of the training error. The green lines show the Monte Carlo simulations of the training error minus the random noise. When the squared bias is added to the variance, the sum is exactly as the mean of the Monte Carlo simulations. As the random error is unknown, the actual training MSE lies slighty higher. The bias decreases as the model complexity increases as the model is better to explain the variance. However, as the each model fits the training data better and better, difference between the models increases, and so the variance increases.\relax }{figure.caption.14}{}}
\newlabel{fig:biastrain@cref}{{[figure][3][2]II.3}{[1][4][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Implementation Details}{5}{section*.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {II.4}{\ignorespaces  The same bias-variance decomposition as in~\cref  {fig:biastrain} but on the test MSE.\spacefactor \@m {} Adding the square bias to the variance matches the Monte Carlo mean of test MSE exactly. In difference to the training decomposition, the test bias increases after an initial decrease, the tell-tell sign of overfitting, and the variance increases faster at high complexity.\relax }}{6}{figure.caption.15}\protected@file@percent }
\newlabel{fig:biastest}{{II.4}{6}{The same bias-variance decomposition as in~\cref {fig:biastrain} but on the test MSE.\@ Adding the square bias to the variance matches the Monte Carlo mean of test MSE exactly. In difference to the training decomposition, the test bias increases after an initial decrease, the tell-tell sign of overfitting, and the variance increases faster at high complexity.\relax }{figure.caption.15}{}}
\newlabel{fig:biastest@cref}{{[figure][4][2]II.4}{[1][4][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {III.1}{\ignorespaces  Visualization of the Franke function in the domain \(0 \leq x, y \leq 1\) colored by height. A random sample of 100 points are also shown.\relax }}{6}{figure.caption.19}\protected@file@percent }
\newlabel{fig:franke}{{III.1}{6}{Visualization of the Franke function in the domain \(0 \leq x, y \leq 1\) colored by height. A random sample of 100 points are also shown.\relax }{figure.caption.19}{}}
\newlabel{fig:franke@cref}{{[figure][1][3]III.1}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {III.2}{\ignorespaces  Digital terrain data over eastern Norway. The plot on the right is down-sampled by a factor \(50\). The terrain is colored by height using the \href  {https://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html}{turbo colormap}.\relax }}{6}{figure.caption.21}\protected@file@percent }
\newlabel{fig:terrain}{{III.2}{6}{Digital terrain data over eastern Norway. The plot on the right is down-sampled by a factor \(50\). The terrain is colored by height using the \href {https://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html}{turbo colormap}.\relax }{figure.caption.21}{}}
\newlabel{fig:terrain@cref}{{[figure][2][3]III.2}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.1}{\ignorespaces  MSE of all models with orders up to \(x^{5}\) and \(y^{5}\) with all interactions. The highest order of \(x\) is visualized by color and highest order of \(y\) is visualized by the circle size. Since many models have the same degree of freedom, points overlap. The clear trend is that more complex models give lower MSE.\spacefactor \@m {} The test MSE is not shown.\relax }}{6}{figure.caption.26}\protected@file@percent }
\newlabel{fig:olscgrouped}{{IV.1}{6}{MSE of all models with orders up to \(x^{5}\) and \(y^{5}\) with all interactions. The highest order of \(x\) is visualized by color and highest order of \(y\) is visualized by the circle size. Since many models have the same degree of freedom, points overlap. The clear trend is that more complex models give lower MSE.\@ The test MSE is not shown.\relax }{figure.caption.26}{}}
\newlabel{fig:olscgrouped@cref}{{[figure][1][4]IV.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results and Discussion}{6}{section*.23}\protected@file@percent }
\newlabel{sec:Discussion}{{IV}{6}{}{section*.23}{}}
\newlabel{sec:Discussion@cref}{{[section][4][]IV}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Modeling the Franke Function}{6}{section*.24}\protected@file@percent }
\newlabel{sec:model-franke-funct}{{IV\tmspace  +\thinmuskip {.1667em}A}{6}{}{section*.24}{}}
\newlabel{sec:model-franke-funct@cref}{{[subsection][1][4]IV\tmspace  +\thinmuskip {.1667em}A}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary Least Squares}{6}{section*.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {IV.2}{\ignorespaces  The mean square error for training, testing and out of sample testing using 5 fold cross validation on a sample of 100 observations. The training error has error bars showing the CV distribution, while for the others the mean of the cross validations are shown as their variance is massive. All models of degree up to \(5\) including interactions are included. The outsample testing MSE is done on an independent sample of 1000 observations. The training error is always decreasing and underestimating the test error. At around \(df = 20\) the models begin to overfit and both the test and outsample test error increases. The y-axis has been cut as the last points have errors two magnitudes greater. The lines mark the best model for each error set using the ``one standard error rule''. \relax }}{7}{figure.caption.27}\protected@file@percent }
\newlabel{fig:olsmse}{{IV.2}{7}{The mean square error for training, testing and out of sample testing using 5 fold cross validation on a sample of 100 observations. The training error has error bars showing the CV distribution, while for the others the mean of the cross validations are shown as their variance is massive. All models of degree up to \(5\) including interactions are included. The outsample testing MSE is done on an independent sample of 1000 observations. The training error is always decreasing and underestimating the test error. At around \(df = 20\) the models begin to overfit and both the test and outsample test error increases. The y-axis has been cut as the last points have errors two magnitudes greater. The lines mark the best model for each error set using the ``one standard error rule''. \relax }{figure.caption.27}{}}
\newlabel{fig:olsmse@cref}{{[figure][2][4]IV.2}{[1][7][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.3}{\ignorespaces  \(R^{2}\) coefficient plotted against degree of freedom of OLS for both test and training sets. It is the inverted version of~\cref  {fig:olsmse}\relax }}{7}{figure.caption.28}\protected@file@percent }
\newlabel{fig:olsr2}{{IV.3}{7}{\(R^{2}\) coefficient plotted against degree of freedom of OLS for both test and training sets. It is the inverted version of~\cref {fig:olsmse}\relax }{figure.caption.28}{}}
\newlabel{fig:olsr2@cref}{{[figure][3][4]IV.3}{[1][7][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.4}{\ignorespaces  95\% confidence interval for the best model as estimated by the ``one standard error'' rule for test MSE.\spacefactor \@m {} The constant and \(x\) term are very significant, and so is the interaction term \(xy\). all of the \(y\)-terms have larger CI.\relax }}{7}{figure.caption.29}\protected@file@percent }
\newlabel{fig:olsci}{{IV.4}{7}{95\% confidence interval for the best model as estimated by the ``one standard error'' rule for test MSE.\@ The constant and \(x\) term are very significant, and so is the interaction term \(xy\). all of the \(y\)-terms have larger CI.\relax }{figure.caption.29}{}}
\newlabel{fig:olsci@cref}{{[figure][4][4]IV.4}{[1][7][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.5}{\ignorespaces  The coefficients for each term of each model for OLS.\spacefactor \@m {} As the model becomes more and more complex with larger degrees of freedom, the coefficients fan out, becoming highly negative or highly positive, even switching sign from one model to the next. \relax }}{8}{figure.caption.30}\protected@file@percent }
\newlabel{fig:olscoeff}{{IV.5}{8}{The coefficients for each term of each model for OLS.\@ As the model becomes more and more complex with larger degrees of freedom, the coefficients fan out, becoming highly negative or highly positive, even switching sign from one model to the next. \relax }{figure.caption.30}{}}
\newlabel{fig:olscoeff@cref}{{[figure][5][4]IV.5}{[1][7][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.6}{\ignorespaces  Effective degrees of freedom computed numerically as the trace of \(\vb {H}\) versus the expected degrees of freedom \(p\). For high \(p\) the effective degrees of freedom deviate, becoming progressively lower.\relax }}{8}{figure.caption.31}\protected@file@percent }
\newlabel{fig:effectivedf}{{IV.6}{8}{Effective degrees of freedom computed numerically as the trace of \(\vb {H}\) versus the expected degrees of freedom \(p\). For high \(p\) the effective degrees of freedom deviate, becoming progressively lower.\relax }{figure.caption.31}{}}
\newlabel{fig:effectivedf@cref}{{[figure][6][4]IV.6}{[1][8][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.7}{\ignorespaces  Condition number of the matrix \(\vb {X}^{T}\vb {X}\) for larger \(p\), i.e. more columns. A matrix with interactions follows the same pattern as one without, constantly increasing for larger \(p\). The apparent plateau at \(10^{18}\) is due to the y-axis being restricted as terms blow up to \(10^{50}\)\relax }}{8}{figure.caption.32}\protected@file@percent }
\newlabel{fig:condition}{{IV.7}{8}{Condition number of the matrix \(\vb {X}^{T}\vb {X}\) for larger \(p\), i.e. more columns. A matrix with interactions follows the same pattern as one without, constantly increasing for larger \(p\). The apparent plateau at \(10^{18}\) is due to the y-axis being restricted as terms blow up to \(10^{50}\)\relax }{figure.caption.32}{}}
\newlabel{fig:condition@cref}{{[figure][7][4]IV.7}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge Regularization}{8}{section*.34}\protected@file@percent }
\newlabel{sec:ridge-regularization}{{IV\tmspace  +\thinmuskip {.1667em}A\tmspace  +\thinmuskip {.1667em}2}{8}{}{section*.34}{}}
\newlabel{sec:ridge-regularization@cref}{{[subsubsection][2][4,1]IV\tmspace  +\thinmuskip {.1667em}A\tmspace  +\thinmuskip {.1667em}2}{[1][8][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.8}{\ignorespaces  Correlation matrix for each term of the design matrix. The higher the degree, the higher the correlation with almost all other terms, especially for the interaction terms.\relax }}{9}{figure.caption.33}\protected@file@percent }
\newlabel{fig:correlation}{{IV.8}{9}{Correlation matrix for each term of the design matrix. The higher the degree, the higher the correlation with almost all other terms, especially for the interaction terms.\relax }{figure.caption.33}{}}
\newlabel{fig:correlation@cref}{{[figure][8][4]IV.8}{[1][8][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.9}{\ignorespaces  Condition number of \(\vb {X}^{T}\vb {X} + \lambda \vb {I}\) plotted against number of columns \(p\) in \(\vb {X}\). The OLS has \(\lambda = 0\) and experiences larger condition numbers for larger \(p\). Ridge uses \(\lambda = 10^{-2}\) causing the matrices to be better conditioned with the condition number leveling off at \(p\approx 10\)\relax }}{9}{figure.caption.35}\protected@file@percent }
\newlabel{fig:condridge}{{IV.9}{9}{Condition number of \(\vb {X}^{T}\vb {X} + \lambda \vb {I}\) plotted against number of columns \(p\) in \(\vb {X}\). The OLS has \(\lambda = 0\) and experiences larger condition numbers for larger \(p\). Ridge uses \(\lambda = 10^{-2}\) causing the matrices to be better conditioned with the condition number leveling off at \(p\approx 10\)\relax }{figure.caption.35}{}}
\newlabel{fig:condridge@cref}{{[figure][9][4]IV.9}{[1][8][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.10}{\ignorespaces  The MSE of Ridge regression as a function of hyperparameter \(\lambda \). In the beginning the variability is high, but decreases quickly to a significant minimum at \(\lambda \approx 5\times 10^{-2}\) in agreement with the out of sample error \textit  {Pred})\relax }}{9}{figure.caption.36}\protected@file@percent }
\newlabel{fig:ridgemse}{{IV.10}{9}{The MSE of Ridge regression as a function of hyperparameter \(\lambda \). In the beginning the variability is high, but decreases quickly to a significant minimum at \(\lambda \approx 5\times 10^{-2}\) in agreement with the out of sample error \textit {Pred})\relax }{figure.caption.36}{}}
\newlabel{fig:ridgemse@cref}{{[figure][10][4]IV.10}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.11}{\ignorespaces  As the Ridge regularization parameter \(\lambda \) increases all terms are penalized and shrunk towards zero. The plot is symmetric about zero, showing that the coefficients change sign from time to time.\relax }}{9}{figure.caption.37}\protected@file@percent }
\newlabel{fig:ridgecoeff}{{IV.11}{9}{As the Ridge regularization parameter \(\lambda \) increases all terms are penalized and shrunk towards zero. The plot is symmetric about zero, showing that the coefficients change sign from time to time.\relax }{figure.caption.37}{}}
\newlabel{fig:ridgecoeff@cref}{{[figure][11][4]IV.11}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Lasso Regularization}{9}{section*.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {IV.12}{\ignorespaces  The coefficient of each term plotted against the Ridge hyperparameter. The top plot uses a linear scale to show how fast the large coefficients are shrunk to zero even with a zoomed in y-axis, while the more intricate behavior is seen in the lower plot user symmetric logarithmic y-axis. The lower degree terms are smaller and shrunk slower than the higher degree terms. The higher terms also have large collinearity, causing them to dance around zero as their explained variability is explained by other correlated coefficients.\relax }}{10}{figure.caption.38}\protected@file@percent }
\newlabel{fig:ridgecoeffevo}{{IV.12}{10}{The coefficient of each term plotted against the Ridge hyperparameter. The top plot uses a linear scale to show how fast the large coefficients are shrunk to zero even with a zoomed in y-axis, while the more intricate behavior is seen in the lower plot user symmetric logarithmic y-axis. The lower degree terms are smaller and shrunk slower than the higher degree terms. The higher terms also have large collinearity, causing them to dance around zero as their explained variability is explained by other correlated coefficients.\relax }{figure.caption.38}{}}
\newlabel{fig:ridgecoeffevo@cref}{{[figure][12][4]IV.12}{[1][9][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.13}{\ignorespaces  The MSE of Lasso regression as function of hyperparameter \(\alpha \). In contrast to Ridge~\cref  {fig:ridgemse} neither the test MSE or training MSE decrease with any significance. Once the hyperparameter gets too large, all terms are set to zero, causing the error to plateau. The out of sample error \textit  {Pred} is confusingly enough smaller than both test and training error.\relax }}{10}{figure.caption.40}\protected@file@percent }
\newlabel{fig:lassomse}{{IV.13}{10}{The MSE of Lasso regression as function of hyperparameter \(\alpha \). In contrast to Ridge~\cref {fig:ridgemse} neither the test MSE or training MSE decrease with any significance. Once the hyperparameter gets too large, all terms are set to zero, causing the error to plateau. The out of sample error \textit {Pred} is confusingly enough smaller than both test and training error.\relax }{figure.caption.40}{}}
\newlabel{fig:lassomse@cref}{{[figure][13][4]IV.13}{[1][10][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.14}{\ignorespaces  The coefficient of each term as the Lasso regularization parameter increases. The highest terms are penalized more heavily than the low, causing them to quickly go to zero. Note also the jumps that happen when a term is temporarily set to zero before being turned on again later. Note the scale different of the x-axis compared to~\cref  {fig:ridgecoeff}\relax }}{10}{figure.caption.41}\protected@file@percent }
\newlabel{fig:lassocoeff}{{IV.14}{10}{The coefficient of each term as the Lasso regularization parameter increases. The highest terms are penalized more heavily than the low, causing them to quickly go to zero. Note also the jumps that happen when a term is temporarily set to zero before being turned on again later. Note the scale different of the x-axis compared to~\cref {fig:ridgecoeff}\relax }{figure.caption.41}{}}
\newlabel{fig:lassocoeff@cref}{{[figure][14][4]IV.14}{[1][10][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Comparison}{10}{section*.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {IV.15}{\ignorespaces  The evolution of the coefficients as function of increasing Lasso hyperparameter \(\alpha \). In the beginning all many high terms have been heavily penalized or set to zero, giving the apparent ``inversion'' of colors when compared to~\cref  {fig:ridgecoeffevo}. The coefficients also do not cross the zero line in contrast to Ridge.\relax }}{11}{figure.caption.42}\protected@file@percent }
\newlabel{fig:lassocoeff_evo}{{IV.15}{11}{The evolution of the coefficients as function of increasing Lasso hyperparameter \(\alpha \). In the beginning all many high terms have been heavily penalized or set to zero, giving the apparent ``inversion'' of colors when compared to~\cref {fig:ridgecoeffevo}. The coefficients also do not cross the zero line in contrast to Ridge.\relax }{figure.caption.42}{}}
\newlabel{fig:lassocoeff_evo@cref}{{[figure][15][4]IV.15}{[1][10][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.16}{\ignorespaces  The relationship between the total number of terms set to zero to the increasing regularization parameter. Even at very low \(\alpha \) some terms are set to zero. For high \(\alpha \) all terms are set to zero.\relax }}{11}{figure.caption.43}\protected@file@percent }
\newlabel{fig:zeros}{{IV.16}{11}{The relationship between the total number of terms set to zero to the increasing regularization parameter. Even at very low \(\alpha \) some terms are set to zero. For high \(\alpha \) all terms are set to zero.\relax }{figure.caption.43}{}}
\newlabel{fig:zeros@cref}{{[figure][16][4]IV.16}{[1][10][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Terrain Data}{11}{section*.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {IV.17}{\ignorespaces  Comparisons of the different regression methods applied to the Franke function. The Franke function is shown in the top left. The OLS training has overfitted, giving good fit for a few regions while others are utterly wrong and the boundaries goes to infinity. The OLS test is much more conservative, using a low degree polynomial. Ridge penalizes the higher coefficients while still retaining their explaining power, hence better at recreating the features of the Franke function. Some boundary issues are seen on the right edge. The Lasso test MSE recommends the smallest possible \(\alpha \) while the out of sample MSE prefers a higher \(\alpha \), their results shown at the bottom. The higher \(\alpha \) causes features of the plot to be more washed out.\relax }}{11}{figure.caption.45}\protected@file@percent }
\newlabel{fig:compare}{{IV.17}{11}{Comparisons of the different regression methods applied to the Franke function. The Franke function is shown in the top left. The OLS training has overfitted, giving good fit for a few regions while others are utterly wrong and the boundaries goes to infinity. The OLS test is much more conservative, using a low degree polynomial. Ridge penalizes the higher coefficients while still retaining their explaining power, hence better at recreating the features of the Franke function. Some boundary issues are seen on the right edge. The Lasso test MSE recommends the smallest possible \(\alpha \) while the out of sample MSE prefers a higher \(\alpha \), their results shown at the bottom. The higher \(\alpha \) causes features of the plot to be more washed out.\relax }{figure.caption.45}{}}
\newlabel{fig:compare@cref}{{[figure][17][4]IV.17}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.18}{\ignorespaces  Training and test MSE of ordinary least squares regression using \(100\) observations of the downsampled terrain. Note the contrast error bars of the training data to the error bars in~\cref  {fig:olsmse} indicating data with higher variance. This causes the best training model to be more conservative, not choosing the one with most terms. The variance of the test data is very large, so the significantly best model is the simplest possible\relax }}{12}{figure.caption.47}\protected@file@percent }
\newlabel{fig:geo_reg_mse}{{IV.18}{12}{Training and test MSE of ordinary least squares regression using \(100\) observations of the downsampled terrain. Note the contrast error bars of the training data to the error bars in~\cref {fig:olsmse} indicating data with higher variance. This causes the best training model to be more conservative, not choosing the one with most terms. The variance of the test data is very large, so the significantly best model is the simplest possible\relax }{figure.caption.47}{}}
\newlabel{fig:geo_reg_mse@cref}{{[figure][18][4]IV.18}{[1][11][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.19}{\ignorespaces  The MSE of Ridge as a function of regularization parameter for the downsampled terrain using \(15\) degree polynomials with all interactions. For low values the model overfits, giving an enormous spike with high variability. The situation improves for \(\lambda > 10^{-5}\) with the test MSE giving the optimal model at \(\lambda \approx 0.9\) before increasing once the penalty decreases the coefficients too much.\relax }}{12}{figure.caption.48}\protected@file@percent }
\newlabel{fig:geo_ridge_mse}{{IV.19}{12}{The MSE of Ridge as a function of regularization parameter for the downsampled terrain using \(15\) degree polynomials with all interactions. For low values the model overfits, giving an enormous spike with high variability. The situation improves for \(\lambda > 10^{-5}\) with the test MSE giving the optimal model at \(\lambda \approx 0.9\) before increasing once the penalty decreases the coefficients too much.\relax }{figure.caption.48}{}}
\newlabel{fig:geo_ridge_mse@cref}{{[figure][19][4]IV.19}{[1][11][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.20}{\ignorespaces  The MSE of Lasso as a function of regularization parameter for the downsampled terrain using \(15\) degree polynomials with all interactions. Even at very small parameter values the largest coefficients are set to zero. Afterwards increasing the parameter has very little effect before too many coefficients get set to zero and the MSE rises. As a result, the optimal model uses the smallest possible value of \(\alpha \). The plateau coming up on the right is left out of the plot\relax }}{12}{figure.caption.49}\protected@file@percent }
\newlabel{fig:geo_lasso_mse}{{IV.20}{12}{The MSE of Lasso as a function of regularization parameter for the downsampled terrain using \(15\) degree polynomials with all interactions. Even at very small parameter values the largest coefficients are set to zero. Afterwards increasing the parameter has very little effect before too many coefficients get set to zero and the MSE rises. As a result, the optimal model uses the smallest possible value of \(\alpha \). The plateau coming up on the right is left out of the plot\relax }{figure.caption.49}{}}
\newlabel{fig:geo_lasso_mse@cref}{{[figure][20][4]IV.20}{[1][11][]12}}
\bibdata{mainNotes,bibliography.bib}
\bibcite{statelem}{{1}{2017}{{Trevor~Hastie}}{{}}}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{13}{section*.51}\protected@file@percent }
\newlabel{sec:Conclusion}{{V}{13}{}{section*.51}{}}
\newlabel{sec:Conclusion@cref}{{[section][5][]V}{[1][13][]13}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{13}{section*.52}\protected@file@percent }
\newlabel{LastBibItem}{{1}{13}{}{section*.52}{}}
\newlabel{LastBibItem@cref}{{[subsection][3][4]IV\tmspace  +\thinmuskip {.1667em}C}{[1][13][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {IV.21}{\ignorespaces  Comparison of the best models according to the ``one standard error'' rule. The models were trained on the downsampled terrain shown in the first panel to the top left. The first two OLS panels were selected from polynomials up to degree \(5\), while Ridge and Lasso used only degree 15 while selecting from different values of their hyperparameters. The last panel to the lower right used polynomials of degree 20 with all interactions and \(1000\) observations in contrast to the others' \(100\). Even in its downsampled incarnation, the terrain has many features that the model are unable to capture. Only the mountains to the north west and the coast lines are captured.\relax }}{14}{figure.caption.50}\protected@file@percent }
\newlabel{fig:geocomp}{{IV.21}{14}{Comparison of the best models according to the ``one standard error'' rule. The models were trained on the downsampled terrain shown in the first panel to the top left. The first two OLS panels were selected from polynomials up to degree \(5\), while Ridge and Lasso used only degree 15 while selecting from different values of their hyperparameters. The last panel to the lower right used polynomials of degree 20 with all interactions and \(1000\) observations in contrast to the others' \(100\). Even in its downsampled incarnation, the terrain has many features that the model are unable to capture. Only the mountains to the north west and the coast lines are captured.\relax }{figure.caption.50}{}}
\newlabel{fig:geocomp@cref}{{[figure][21][4]IV.21}{[1][12][]14}}
\newlabel{LastPage}{{}{14}{}{}{}}
