\section{Method}\label{sec:Method}


\subsection{Implementation Details}

When using linear regression to solve~\eqref{eq:1}, methods from
\texttt{sklearn.linear\_models} were used as they are quicker and more versatile
than my own code written for the first project.

The logistic regression and neural network were written in Julia. Three versions
of gradient descent were written: gradient descent, stochastic gradient descent
and Nesterov gradient descent, all with early stopping. All of these
were tested, confirmable by the example scripts in the git repository.

Due to my
unfamiliarity with the language, I made several critical design flaws. I began
writing the code as I would do in C++ or Python, by object orienting it. Julia,
however, is not designed for OO-programming, and trying to do so rewards you with
inflexible code that is hard to optimize. The type system was abused, so
instead of allowing my code to work on many different types, I constantly
constrained myself to a small subset.

The result of this mess is that a lot of time was spent hunting down bugs and
trying to optimize hard-to-optimize code, wasting time and generating a low quality
product. Two major consequences of this is that the neural net is incapable of
performing linear regression, only classification, and that the code runs
abysmally slow. In order to actually solve the problems of the project, I was
forced to use the Julia package \texttt{Flux.jl} for the linear regression and
grid search over hyperparameters for the classification. 

The reader can confirm that my neural net implementation does work on
classification problems by running
either the example script in the git repository or taking a look in the
notebooks. 

